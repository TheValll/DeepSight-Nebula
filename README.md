# ğŸ¤–âœ¨ DeepSight-Nebula

**DeepSight-Nebula est un projet alliant Robotique & Intelligence Artificielle.**

---

## ğŸŒŒ Pourquoi le nom _DeepSight-Nebula_ ?

- ğŸ” _DeepSight_ : fait rÃ©fÃ©rence Ã  la **profondeur**, en Ã©cho au systÃ¨me de vision de ce projet.
- ğŸŒ  _Nebula_ : en hommage aux **nÃ©buleuses** objets cÃ©leste afin de reflÃ©ter ma passion pour lâ€™espace.

---

## ğŸ¯ Objectifs de DeepSight-Nebula

Le projet a plusieurs ambitions :

- ğŸš€ Mâ€™amÃ©liorer en **robotique** et en **IA** de maniÃ¨re gÃ©nÃ©rale.
- ğŸ“š Servir de base pour un **mÃ©moire de fin de master** en _Big Data & IA_, et potentiellement prÃ©parer un **PhD en Robotique & IA**.
- ğŸ¦¾ Concevoir un systÃ¨me capable de :
  - Piloter un **bras robotique** via une interface (comme un Raspberry Pi ğŸ“).
  - Utiliser un systÃ¨me de **vision** avec camÃ©ras et capteurs.
  - DÃ©ployer un **modÃ¨le dâ€™IA** pour analyser les objets prÃ©sents dans son champ de vision.
  - Ramasser les objets dÃ©tectÃ©s **de maniÃ¨re autonome**â€¦ le tout avec un budget raisonnable ğŸ’°.

---

## ğŸ“ Objectif de ce README

Ce document a deux fonctions principales :

- ğŸ“– RÃ©sumer et clarifier le but du projet pour moi et pour les lecteurs.
- ğŸ—’ï¸ Servir de **journal de bord** pour documenter mes recherches et dÃ©veloppements.

Jâ€™y consignerai :

- Des images ğŸ–¼ï¸
- Des schÃ©mas ğŸ“
- Des explications dÃ©taillÃ©es ğŸ“„
- Des solutions aux problÃ¨mes rencontrÃ©s ğŸ§©
- â€¦ et peut-Ãªtre mÃªme des questions que je me poserai en chemin ğŸ¤”

Je mâ€™appuierai sur **ChatGPT** pour mâ€™aider Ã  rÃ©diger, reformuler et amÃ©liorer ce document tout au long du projet.

---

## ğŸ“” Journal de bord

### ğŸ—“ï¸ 22/07/2025

Jâ€™ai dÃ©butÃ© mes recherches en rÃ©flÃ©chissant Ã  la conception et Ã  la faisabilitÃ© de ce projet.

ğŸ”¹ Dans un premier temps, je me suis penchÃ© sur le choix du bras robotique mais jâ€™ai vite Ã©tÃ© freinÃ© par les prix des modÃ¨les milieu de gamme.  
ğŸ”¹ Ensuite, je me suis plutÃ´t orientÃ© vers la **dÃ©tection et la vision par capteurs** en explorant plusieurs options :

- Lidar 2D ou 3D
- CamÃ©ra RGB-D
- CamÃ©ra stÃ©rÃ©o
- Ou une combinaison de tout cela

Comme je souhaite utiliser un **Raspberry Pi** (qui me servira aussi pour dâ€™autres projets), jâ€™en ai conclu aprÃ¨s plusieurs heures de recherche que je pourrais partir sur :

- ğŸ¥ Une **camÃ©ra ELP stÃ©rÃ©o** Ã  ~125â€¯â‚¬  
  [Lien Amazon](https://www.amazon.fr/ELP-distorsion-Synchronisation-dordinateur-Raspberry/dp/B07FT2GKZS?source=ps-sl-shoppingads-lpcontext&ref_=fplfs&psc=1&smid=A1XYWUUU38OZI5&gQT=1)
- ğŸ“¡ Un **capteur TF-Luna** Ã  ~29â€¯â‚¬  
  [Lien Amazon](https://www.amazon.fr/youyeetoo-TF-Luna-Distance-d%C3%A9tection-industrielle/dp/B088BBJ9SQ)

ğŸ‘‰ Je nâ€™ai pas encore pris la dÃ©cision de les acheter mais cela me donne une idÃ©e de la faisabilitÃ© et du budget.

ğŸ‘‰ Je n'ai Ã©galement pas choisi l'Intel RealSense car ses dimensions sont trop grandes pour Ãªtre fixÃ©es sur un bras robot.

---

### ğŸ”§ RÃ©flexions techniques

ğŸ’¡ Jâ€™ai rapidement compris que la difficultÃ© majeure sera la **calibration et la fusion des donnÃ©es** issues du lidar et des camÃ©ras.  
Je pars donc sur lâ€™idÃ©e dâ€™utiliser **Python ou C++ avec OpenCV (cv2)** avec une prÃ©fÃ©rence pour Python notamment pour lâ€™intÃ©gration des modÃ¨les dâ€™IA.

- OpenCV me permettra de fusionner le flux des deux camÃ©ras et dâ€™obtenir un output image exploitable.
- Câ€™est ici que lâ€™IA entre en jeu : aprÃ¨s quelques recherche je vais probablement utiliser le modÃ¨le **YOLOWorld**, puis Ã  terme entraÃ®ner mon propre modÃ¨le sur un dataset personnalisÃ© (plus tard car câ€™est trÃ¨s chronophage).

Le modÃ¨le dÃ©tectera un objet particulier sur le flux vidÃ©o et retournera ses coordonnÃ©es **x** et **y**.  
Si aucun objet nâ€™est trouvÃ© dans le champ de vision le bras pourra bouger sur son axe pour balayer lâ€™espace.

---

### ğŸ“ Calibration & prÃ©cision

- Il faudra calibrer lâ€™ensemble (camÃ©ras + lidar) via un systÃ¨me de **calibrage extrinsÃ¨que** que je ne maÃ®trise pas encore.
- Pour commencer, je compte utiliser un **servo SG90 + Arduino** pour ajuster lâ€™axe de la camÃ©ra afin de rÃ©duire lâ€™angle entre lâ€™axe central de la camÃ©ra et lâ€™objet Ã  environ 2Â° (lâ€™angle max du TF-Luna placÃ© juste en dessous de la camÃ©ra). Cela permettra de vÃ©rifier la profondeur avec plus de prÃ©cision grÃ¢ce au lidar.
- Je recalculerai ensuite les coordonnÃ©es **x**, **y** de lâ€™objet en tenant compte de la calibration.
- Dans le futur le SG90 sera remplacÃ© par un mouvement de la base du bras.

---

### ğŸ¤– Mouvement & trajectoire

Avec les coordonnÃ©es **x, y, z** je pourrai dÃ©terminer un chemin pour le bras afin quâ€™il rÃ©cupÃ¨re lâ€™objet sans doute en utilisant des vecteurs et des contrÃ´leurs.  
Ces rÃ©flexions mâ€™ont permis de voir que jâ€™ai encore du temps et des alternatives avant dâ€™investir dans un Raspberry Pi et un bras robot.

---

### ğŸ’° Budget estimÃ©

| Ã‰quipement                     | Prix approx. |
| ------------------------------ | ------------ |
| CamÃ©ra stÃ©rÃ©o ELP              | 125â€¯â‚¬        |
| Lidar TF-Luna                  | 29â€¯â‚¬         |
| Bras robot (entrÃ©e/moyen)      | 60â€“200â€¯â‚¬     |
| Raspberry Pi (+ alim, modules) | 150â€¯â‚¬        |

**Total estimÃ© : ~400â€¯â‚¬**

---

### ğŸ”œ Prochaines Ã©tapes

Je vais commencer par :

- Me familiariser avec le modÃ¨le **YOLOWorld** et OpenCV.
- Utiliser ma camÃ©ra actuelle (Logitech StreamCam) pour expÃ©rimenter.
- Je dois trouver un moyen de faire tenir ma camÃ©ra sur le SG90 de maniÃ¨re stable.
- Ajuster lâ€™axe de la camÃ©ra pour entrer dans la plage de vision du lidar (~2Â°) aprÃ¨s avoir dÃ©tectÃ© un objet avec **YOLOWorld** et calculer les premiÃ¨res coordonnÃ©es dans OpenCV.

---

### ğŸ—ºï¸ SchÃ©ma

La premiÃ¨re grosse Ã©tape de ce projet sera dâ€™apprendre Ã  utiliser, avec ce que jâ€™ai dÃ©jÃ  Ã  disposition, les outils (OpenCV, modÃ¨le IA, etc.), puis de passer Ã  lâ€™achat du lidar et de la camÃ©ra.  
Lâ€™objectif de cette Ã©tape est dâ€™arriver Ã  obtenir les **coordonnÃ©es 3D dâ€™un objet** dans le champ de vision de la camÃ©ra.

Voici le schÃ©ma de la reprÃ©sentation de cette premiÃ¨re Ã©tape que jâ€™ai en tÃªteâ€¯:

![SchÃ©ma Ã©tape 1](schemas/schema1.png)

---
