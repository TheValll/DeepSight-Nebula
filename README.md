# ğŸ¤–âœ¨ DeepSight-Nebula

---

## ğŸ“‘ Table of Contents / Sommaire

### ğŸ‡¬ğŸ‡§ English Version

- [Why the name _DeepSight-Nebula_?](#-why-the-name-deepsight-nebula)
- [Goals of DeepSight-Nebula](#-goals-of-deepsight-nebula)
- [Purpose of this README](#-purpose-of-this-readme)
- [Logbook (English)](#-logbook)

### ğŸ‡«ğŸ‡· Version FranÃ§aise

- [Pourquoi le nom _DeepSight-Nebula_ ?](#-pourquoi-le-nom-deepsight-nebula-)
- [Objectifs de DeepSight-Nebula](#-objectifs-de-deepsight-nebula)
- [Objectif de ce README](#-objectif-de-ce-readme)
- [Journal de bord](#-journal-de-bord)

---

## ğŸ‡¬ğŸ‡§ English Version

**DeepSight-Nebula is a project combining Robotics & Artificial Intelligence.**

---

## ğŸŒŒ Why the name _DeepSight-Nebula_?

- ğŸ” _DeepSight_: refers to **depth**, reflecting the project's vision system.
- ğŸŒ  _Nebula_: refers to **nebulas**, celestial objects, to reflect my passion for space.

---

## ğŸ¯ Goals of DeepSight-Nebula

The project has several goals:

- ğŸš€ To improve my skills in **robotics** and **AI** in general.
- ğŸ“š To serve as a basis for a **master's thesis** in _Big Data & AI_, and possibly prepare for a **PhD in Robotics & AI**.
- ğŸ¦¾ To design a system that can:
  - Control a **robotic arm** through an interface (like a Raspberry Pi ğŸ“).
  - Use a **vision** system with cameras and sensors.
  - Deploy an **AI model** to analyze objects in its field of view.
  - Pick up detected objects **autonomously**... all within a reasonable budget ğŸ’°.

---

## ğŸ“ Purpose of this README

This document has two main functions:

- ğŸ“– To summarize and clarify the project's goal for myself and for readers.
- ğŸ—’ï¸ To serve as a **logbook** to document my research and development.

I will record:

- Images ğŸ–¼ï¸
- Diagrams ğŸ“
- Detailed explanations ğŸ“„
- Solutions to problems encountered ğŸ§©
- ... and maybe even questions I ask myself along the way ğŸ¤”

I will use **ChatGPT** to help me rephrase and improve this document throughout the project.

---

## ğŸ“” Logbook

### ğŸ—“ï¸ 07/22/2025

I started my research by thinking about the design and feasibility of this project.

ğŸ”¹ First, I looked into choosing a robotic arm but was quickly put off by the prices of mid-range models.
ğŸ”¹ Then, I focused more on **sensor-based detection and vision**, exploring several options:

- 2D or 3D Lidar
- RGB-D Camera
- Stereo Camera
- Or a combination of all of these

Since I want to use a **Raspberry Pi** (which I will also use for other projects), I concluded after several hours of research that I could start with:

- ğŸ¥ An **ELP stereo camera** for ~â‚¬125
  [Amazon Link](https://www.amazon.fr/ELP-distorsion-Synchronisation-dordinateur-Raspberry/dp/B07FT2GKZS?source=ps-sl-shoppingads-lpcontext&ref_=fplfs&psc=1&smid=A1XYWUUU38OZI5&gQT=1)
- ğŸ“¡ A **TF-Luna sensor** for ~â‚¬29
  [Amazon Link](https://www.amazon.fr/youyeetoo-TF-Luna-Distance-d%C3%A9tection-industrielle/dp/B088BBJ9SQ)

ğŸ‘‰ I haven't decided to buy them yet, but this gives me an idea of the feasibility and budget.

ğŸ‘‰ I also did not choose the Intel RealSense (the most well-known stereo camera) because it is too large to be mounted on a robotic arm.

---

### ğŸ”§ Technical Thoughts

ğŸ’¡ I quickly realized that the main difficulty will be the **calibration and fusion of data** from the lidar and cameras.
So, I'm planning to use **Python or C++ with OpenCV (cv2)**, with a preference for Python, especially for integrating AI models.

- OpenCV will allow me to merge the stream from the two cameras and get a usable image output.
- This is where AI comes in: after some research, I will probably use the **YOLOWorld** model, and later train my own model on a custom dataset (later, as it's very time-consuming).

The model will detect a specific object in the video stream and return its **x** and **y** coordinates.
If no object is found in the field of view, the arm can move on its axis to scan the area.

---

### ğŸ“ Calibration & Precision

- The entire system (cameras + lidar) will need to be calibrated using an **extrinsic calibration** system, which I don't master yet.
- To start, I plan to use an **SG90 servo + Arduino** to adjust the camera's axis to reduce the angle between the camera's central axis and the object to about 2Â° (the max angle of the TF-Luna placed just below the camera). This will allow checking the depth more accurately with the lidar.
- I will then recalculate the object's **x**, **y** coordinates, taking the calibration into account.
- In the future, the SG90 will be replaced by a movement of the arm's base.

---

### ğŸ¤– Movement & Trajectory

With the **x, y, z** coordinates, I can determine a path for the arm to retrieve the object, probably using vectors and controllers.
These thoughts helped me see that I still have time and alternatives before investing in a Raspberry Pi and a robotic arm.

---

### ğŸ’° Estimated Budget

| Equipment                       | Approx. Price |
| ------------------------------- | ------------- |
| ELP Stereo Camera               | â‚¬125          |
| TF-Luna Lidar                   | â‚¬29           |
| Robotic Arm (entry/mid)         | â‚¬60â€“200       |
| Raspberry Pi (+ power, modules) | â‚¬150          |

**Estimated Total: ~â‚¬400**

---

### ğŸ”œ Next Steps

I will start by:

- Getting familiar with the **YOLOWorld** model and OpenCV.
- Using my current camera (Logitech StreamCam) to experiment.
- I need to find a way to stably mount my camera on the SG90.
- Adjusting the camera's axis to get within the lidar's FOV (~2Â°) after detecting an object with **YOLOWorld** and calculating the first coordinates in OpenCV.

---

### ğŸ—ºï¸ Diagram

The first big step of this project will be to learn how to use the tools (OpenCV, AI model, etc.) with what I already have, then move on to buying the lidar and camera.
The goal of this step is to be able to get the **3D coordinates of an object** in the camera's field of view.

Here is the diagram of this first step as I imagine it:

![Step 1 Diagram](schemas/schema1.png)

---

### ğŸ—“ï¸ 11/03/2025

It's been 4 months since I wrote an update on this document, mainly due to lack of time and an intense learning phase.

---

### ğŸ› ï¸ Learning & Development

These last few months have been dedicated to learning new skills and starting development on the robot.

- I purchased the xArm Esp32 robot arm from [Hiwonder](https://www.hiwonder.com/products/xarm-esp32?variant=39662930067543)

- **ROS2 Training**: I started learning **ROS2** by following this [YouTube tutorial](https://www.youtube.com/watch?v=Gg25GfA456o&t). I learned about the concepts of _nodes_, _publishers_, _subscribers_, _clients_, _servers_, and _actions_.
- **Robot Control**: Since the **xArm ESP32** arm doesn't have manufacturer software to make development easier, I had to learn how to retrieve the information passing through the USB ports.
  - Using the **COM8 Monitoring Session** software, I was able to analyze the commands sent by the robot's basic software.
  - This helped me understand which commands to send to control it. You can find this development in the `utils/xarm_esp32_init.py` file, which manages basic actions.
- **Math & AI**: I am currently taking math courses on Coursera ([Mathematics for Machine Learning and Data Science by DeepLearning.AI](https://www.coursera.org/specializations/mathematics-machine-learning-data-science)). As a Data & AI master's student, these courses will be essential, especially for my long-term goal of creating my own AI model.

---

### ğŸ—ï¸ Architecture & Design

My thoughts on the hardware and software architecture have evolved a lot.

- **ROS Architecture Diagram**: I had made an initial diagram of my ROS architecture. It was just a starting point and definitely not a final solution.
  ![Architecture Diagram](schemas/schema2.png)
- **Dropping the Lidar**: In the meantime, I decided to no longer use a Lidar sensor (like the TF-Luna).
  - The main reason is that the stereo camera will be sufficient to generate a **DepthMap** using OpenCV.
  - Also, the Lidar required very precise alignment (angle < 2Â°) between the camera, the gripper, and the object.
  - My first tests showed that the robot is not precise or robust enough for this. The weight, latency, and servo precision caused oscillations (back-and-forth movements) during calibration attempts, without ever succeeding. So, I will focus only on the stereo camera.
- **Camera Positioning**: I plan to install the stereo camera just below the gripper, attached to the servo that controls it.
  ![Camera Positioning Diagram](schemas/schema3.png)

---

### ğŸ’° Hardware & Budget

The estimated budget is on track with the following purchases:

| Equipment              | Price     | Link                                                                            |
| ---------------------- | --------- | ------------------------------------------------------------------------------- |
| xArm ESP32 Robotic Arm | â‚¬229.99   | [Hiwonder](https://www.hiwonder.com/products/xarm-esp32?variant=39662930067543) |
| ELP Stereo USB Camera  | â‚¬125      | [Amazon](https://www.amazon.fr/dp/B07FT2GKZS)                                   |
| Raspberry Pi           | To buy    |                                                                                 |
| **Total (current)**    | **~â‚¬355** |                                                                                 |

These purchases fit well within the estimated â‚¬400 budget I mentioned earlier.

---

### ğŸ—ºï¸ Outline & Goals (Update)

With these new elements, the project's outline is becoming clearer:

1.  **Goal**: Autonomously pick up an object (a tennis ball to start).
2.  **AI (Vision)**: I will use **YOLOv8** (for its performance) for object detection.
3.  **Vision (Depth)**: The **stereo camera** and **OpenCV** will allow me to generate a DepthMap and get the object's **3D coordinates**.
4.  **Movement**: I will learn to use **URDF** files (to model the robot) and the **MoveIt** library (to plan the trajectory) to determine the optimal path to grab the object.
5.  **Future**: Eventually, I want to create my own AI model, possibly based on **reinforcement learning** (either for object recognition or for the complete grasping task).

---

### ğŸ”œ Next Steps

Now that the ideas and technologies are clearer:

- Wait for my stereo camera to be delivered.
- Continue learning ROS2, especially **URDF files** and **MoveIt** (via the same YouTube channel).
- Learn 3D modeling or 3D printing to create a custom camera mount.
  - I will contact my school's innovation club, which offers training.
  - Alternatively, I will adapt an existing 3D model, like [this one](https://makerworld.com/en/models/27135-raspberry-camera-mount?from=search), and contact its creator to see if it's compatible.
- Redraw a more detailed global project architecture diagram.
- Try to contact Edouard Renard (robotics instructor) to ask his opinion on my architecture and ideas before developing.

---

### ğŸ—“ï¸ 11/08/2025

Progress continues, with advances in the vision part and the robot's modeling.

---

### ğŸ› ï¸ Development & Vision

- **Stereo Camera**: I received my ELP stereo camera.
- **Test Script**: I was able to write a first script `utils/stereo_camera.py` that lets me initialize the camera and display the video feeds from both cameras.

---

### ğŸ—ï¸ Architecture (Update)

I simplified my main architecture to make it more achievable for a first version.

- **New Architecture**:
  ![Architecture Diagram](schemas/schema4.png)
- **Current Limitations**: In this setup, the robot is not capable of real-time operation. It will have to wait for the object to be picked up to be still, and to remain still during the entire movement.
- **V1 Goal**: I will stick to this approach for the first version of the project. Real-time trajectory correction will be an improvement for the future.

---

### ğŸ¦¾ Modeling & URDF

Creating the robot's digital twin in ROS2 was a major and complex step.

- **URDF File**: I learned to write a `.xacro` file and build a ROS2 package.
- **3D Model**: The Hiwonder company provided me with the `.stp` file for my robot, which I opened in **Fusion360**.
- **Assembly**: I had to sort and assemble the 309 base parts into logical "components" (gripper, base, limb1, etc.) and create the joints between them.
- **Major Difficulty**: The default export script ([fusion2urdf](https://github.com/syuntoku14/fusion2urdf/tree/master)) does not accept Fusion's "as-built" joints. But I couldn't use simple joints because the parts were already assembled in the base `.stp` file.
- **Solution**: After 3 days of searching, I found a [GitHub issue](https://github.com/syuntoku14/fusion2urdf/issues/78) describing the same problem.
  - A big thank you to **Colin Fuelberth** ([@Infinite-Echo](https://github.com/Infinite-Echo)) who forked and adapted the script to support "as-built" joints!
  - **Script used**: [Infinite-Echo/ROS2_fusion2URDF](https://github.com/Infinite-Echo/ROS2_fusion2URDF/tree/URDF_Exporter_asBuilt_Support)
- **Result**: I was finally able to export a complete ROS2 package with a `.xacro` file that describes my robot arm, which is in the `modelisations/robot/xArm32_description` folder. This URDF does not include the gripper's opening and closing. I will adapt it when the time comes; the main goal was to have the basics and learn the Fusion software.

---

### ğŸ”œ Next Steps

- I am currently following a **Blender** tutorial which will allow me to model and 3D print my own camera mount in the future.
- **Switch to Docker or Linux** (with a dual boot) for ROS2. I am way too limited with Windows, which doesn't make it easy, especially for opening and viewing my `.xacro`.

---

### ğŸ—“ï¸ 11/14/2025

I've continued working on the URDF model these last few days. I described the joints between the arm's segments (limbs) with limits, except for the gripper because understanding _mimics_ is a bit too complex for me right now. I'll probably control the gripper directly via the ESP32.

In the `app` folder, the first `joint_state_publisher_node` package is functional with the instructions.
I switched to Docker for ROS2 Humble, but I'm facing a frames-per-second (FPS) problem during simulation in RViz or Gazebo, which makes the user experience unpleasant.

You can find a zip file here containing the `.stp` provided by Hiwonder and the Fusion360 file I edited, in case modifications or improvements are needed: [Google Drive Link](https://drive.google.com/file/d/1qIVWolMBeF4Z5x8Bm8aadgIRzJTUaZLs/view?usp=sharing)

Thanks to this important step, I can visualize the main joints of the arm via RViz or Gazebo.
I don't think I will use Gazebo in this project.

![RViz result](schemas/schema5.gif)

---

### ğŸ”œ Next Steps

- I am currently following a **Blender** tutorial that will allow me to model and 3D print my own camera mount in the future.
- Learn about **ros2_control** and **MoveIt**.
- Learn to use _mimic joints_, which would allow me to move the gripper in RViz as well. I have already started creating the necessary joints in Fusion for this, but the URDF format does not accept closed joint loops.

![Fusion360 gripper visualization](schemas/schema6.gif)

---

## ğŸ‡«ğŸ‡· Version FranÃ§aise

# ğŸ¤–âœ¨ DeepSight-Nebula

**DeepSight-Nebula est un projet alliant Robotique & Intelligence Artificielle.**

---

## ğŸŒŒ Pourquoi le nom _DeepSight-Nebula_ ?

- ğŸ” _DeepSight_ : fait rÃ©fÃ©rence Ã  la **profondeur**, en Ã©cho au systÃ¨me de vision de ce projet.
- ğŸŒ  _Nebula_ : rÃ©fÃ©rence aux **nÃ©buleuses** objets cÃ©leste afin de reflÃ©ter ma passion pour lâ€™espace.

---

## ğŸ¯ Objectifs de DeepSight-Nebula

Le projet a plusieurs ambitions :

- ğŸš€ Mâ€™amÃ©liorer en **robotique** et en **IA** de maniÃ¨re gÃ©nÃ©rale.
- ğŸ“š Servir de base pour un **mÃ©moire de fin de master** en _Big Data & IA_, et potentiellement prÃ©parer un **PhD en Robotique & IA**.
- ğŸ¦¾ Concevoir un systÃ¨me capable de :
  - Piloter un **bras robotique** via une interface (comme un Raspberry Pi ğŸ“).
  - Utiliser un systÃ¨me de **vision** avec camÃ©ras et capteurs.
  - DÃ©ployer un **modÃ¨le dâ€™IA** pour analyser les objets prÃ©sents dans son champ de vision.
  - Ramasser les objets dÃ©tectÃ©s **de maniÃ¨re autonome**â€¦ le tout avec un budget raisonnable ğŸ’°.

---

## ğŸ“ Objectif de ce README

Ce document a deux fonctions principales :

- ğŸ“– RÃ©sumer et clarifier le but du projet pour moi et pour les lecteurs.
- ğŸ—’ï¸ Servir de **journal de bord** pour documenter mes recherches et dÃ©veloppements.

Jâ€™y consignerai :

- Des images ğŸ–¼ï¸
- Des schÃ©mas ğŸ“
- Des explications dÃ©taillÃ©es ğŸ“„
- Des solutions aux problÃ¨mes rencontrÃ©s ğŸ§©
- â€¦ et peut-Ãªtre mÃªme des questions que je me poserai en chemin ğŸ¤”

Je mâ€™appuierai sur **ChatGPT** pour mâ€™aider Ã  reformuler et amÃ©liorer ce document tout au long du projet.

---

## ğŸ“” Journal de bord

### ğŸ—“ï¸ 22/07/2025

Jâ€™ai dÃ©butÃ© mes recherches en rÃ©flÃ©chissant Ã  la conception et Ã  la faisabilitÃ© de ce projet.

ğŸ”¹ Dans un premier temps, je me suis penchÃ© sur le choix du bras robotique mais jâ€™ai vite Ã©tÃ© freinÃ© par les prix des modÃ¨les milieu de gamme.  
ğŸ”¹ Ensuite, je me suis plutÃ´t orientÃ© vers la **dÃ©tection et la vision par capteurs** en explorant plusieurs options :

- Lidar 2D ou 3D
- CamÃ©ra RGB-D
- CamÃ©ra stÃ©rÃ©o
- Ou une combinaison de tout cela

Comme je souhaite utiliser un **Raspberry Pi** (qui me servira aussi pour dâ€™autres projets), jâ€™en ai conclu aprÃ¨s plusieurs heures de recherche que je pourrais partir sur :

- ğŸ¥ Une **camÃ©ra ELP stÃ©rÃ©o** Ã  ~125â€¯â‚¬  
  [Lien Amazon](https://www.amazon.fr/ELP-distorsion-Synchronisation-dordinateur-Raspberry/dp/B07FT2GKZS?source=ps-sl-shoppingads-lpcontext&ref_=fplfs&psc=1&smid=A1XYWUUU38OZI5&gQT=1)
- ğŸ“¡ Un **capteur TF-Luna** Ã  ~29â€¯â‚¬  
  [Lien Amazon](https://www.amazon.fr/youyeetoo-TF-Luna-Distance-d%C3%A9tection-industrielle/dp/B088BBJ9SQ)

ğŸ‘‰ Je nâ€™ai pas encore pris la dÃ©cision de les acheter mais cela me donne une idÃ©e de la faisabilitÃ© et du budget.

ğŸ‘‰ Je n'ai Ã©galement pas choisi l'Intel RealSense (camÃ©ra stÃ©rÃ©o la plus connue) car ses dimensions sont trop grandes pour Ãªtre fixÃ©es sur un bras robot.

---

### ğŸ”§ RÃ©flexions techniques

ğŸ’¡ Jâ€™ai rapidement compris que la difficultÃ© majeure sera la **calibration et la fusion des donnÃ©es** issues du lidar et des camÃ©ras.  
Je pars donc sur lâ€™idÃ©e dâ€™utiliser **Python ou C++ avec OpenCV (cv2)** avec une prÃ©fÃ©rence pour Python notamment pour lâ€™intÃ©gration des modÃ¨les dâ€™IA.

- OpenCV me permettra de fusionner le flux des deux camÃ©ras et dâ€™obtenir un output image exploitable.
- Câ€™est ici que lâ€™IA entre en jeu : aprÃ¨s quelques recherche je vais probablement utiliser le modÃ¨le **YOLOWorld**, puis Ã  terme entraÃ®ner mon propre modÃ¨le sur un dataset personnalisÃ© (plus tard car câ€™est trÃ¨s chronophage).

Le modÃ¨le dÃ©tectera un objet particulier sur le flux vidÃ©o et retournera ses coordonnÃ©es **x** et **y**.  
Si aucun objet nâ€™est trouvÃ© dans le champ de vision le bras pourra bouger sur son axe pour balayer lâ€™espace.

---

### ğŸ“ Calibration & prÃ©cision

- Il faudra calibrer lâ€™ensemble (camÃ©ras + lidar) via un systÃ¨me de **calibrage extrinsÃ¨que** que je ne maÃ®trise pas encore.
- Pour commencer, je compte utiliser un **servo SG90 + Arduino** pour ajuster lâ€™axe de la camÃ©ra afin de rÃ©duire lâ€™angle entre lâ€™axe central de la camÃ©ra et lâ€™objet Ã  environ 2Â° (lâ€™angle max du TF-Luna placÃ© juste en dessous de la camÃ©ra). Cela permettra de vÃ©rifier la profondeur avec plus de prÃ©cision grÃ¢ce au lidar.
- Je recalculerai ensuite les coordonnÃ©es **x**, **y** de lâ€™objet en tenant compte de la calibration.
- Dans le futur le SG90 sera remplacÃ© par un mouvement de la base du bras.

---

### ğŸ¤– Mouvement & trajectoire

Avec les coordonnÃ©es **x, y, z** je pourrai dÃ©terminer un chemin pour le bras afin quâ€™il rÃ©cupÃ¨re lâ€™objet sans doute en utilisant des vecteurs et des contrÃ´leurs.  
Ces rÃ©flexions mâ€™ont permis de voir que jâ€™ai encore du temps et des alternatives avant dâ€™investir dans un Raspberry Pi et un bras robot.

---

### ğŸ’° Budget estimÃ©

| Ã‰quipement                     | Prix approx. |
| ------------------------------ | ------------ |
| CamÃ©ra stÃ©rÃ©o ELP              | 125â€¯â‚¬        |
| Lidar TF-Luna                  | 29â€¯â‚¬         |
| Bras robot (entrÃ©e/moyen)      | 60â€“200â€¯â‚¬     |
| Raspberry Pi (+ alim, modules) | 150â€¯â‚¬        |

**Total estimÃ© : ~400â€¯â‚¬**

---

### ğŸ”œ Prochaines Ã©tapes

Je vais commencer par :

- Me familiariser avec le modÃ¨le **YOLOWorld** et OpenCV.
- Utiliser ma camÃ©ra actuelle (Logitech StreamCam) pour expÃ©rimenter.
- Je dois trouver un moyen de faire tenir ma camÃ©ra sur le SG90 de maniÃ¨re stable.
- Ajuster lâ€™axe de la camÃ©ra pour entrer dans la plage de vision du lidar (~2Â°) aprÃ¨s avoir dÃ©tectÃ© un objet avec **YOLOWorld** et calculer les premiÃ¨res coordonnÃ©es dans OpenCV.

---

### ğŸ—ºï¸ SchÃ©ma

La premiÃ¨re grosse Ã©tape de ce projet sera dâ€™apprendre Ã  utiliser, avec ce que jâ€™ai dÃ©jÃ  Ã  disposition, les outils (OpenCV, modÃ¨le IA, etc.), puis de passer Ã  lâ€™achat du lidar et de la camÃ©ra.  
Lâ€™objectif de cette Ã©tape est dâ€™arriver Ã  obtenir les **coordonnÃ©es 3D dâ€™un objet** dans le champ de vision de la camÃ©ra.

Voici le schÃ©ma de la reprÃ©sentation de cette premiÃ¨re Ã©tape que jâ€™ai en tÃªteâ€¯:

![SchÃ©ma Ã©tape 1](schemas/schema1.png)

---

### ğŸ—“ï¸ 03/11/2025

Cela fait 4 mois que je n'ai pas rÃ©digÃ© de rÃ©capitulatif sur ce document, principalement par manque de temps et en raison d'une phase d'apprentissage intense.

---

### ğŸ› ï¸ Apprentissage & DÃ©veloppement

Ces derniers mois ont Ã©tÃ© consacrÃ©s Ã  l'acquisition de nouvelles compÃ©tences et au premier dÃ©veloppement sur le robot.

- J'ai fait l'achat du bras robot xArm Esp32 de chez [Hiwonder](https://www.hiwonder.com/products/xarm-esp32?variant=39662930067543)

- **Formation ROS2** : J'ai commencÃ© Ã  me former Ã  **ROS2** en suivant ce [tutoriel YouTube](https://www.youtube.com/watch?v=Gg25GfA456o&t). J'ai pu apprÃ©hender les concepts de _nodes_, _publisher_, _subscriber_, _client_, _server_ et _actions_.
- **ContrÃ´le du Robot** : Le bras **xArm ESP32** n'ayant pas de logiciel constructeur facilitant le dÃ©veloppement, j'ai dÃ» apprendre Ã  rÃ©cupÃ©rer les informations transitant via les ports USB.
  - Ã€ l'aide du logiciel **COM8 Monitoring Session**, j'ai pu analyser les commandes envoyÃ©es par le logiciel basique du robot.
  - J'ai ainsi pu comprendre quelles commandes envoyer pour le piloter. Vous trouverez ce dÃ©veloppement dans le fichier `utils/xarm_esp32_init.py` qui gÃ¨re les actions de base.
- **MathÃ©matiques & IA** : Je suis actuellement des cours de mathÃ©matiques sur Coursera ([Mathematics for Machine Learning and Data Science de DeepLearning.AI](https://www.coursera.org/specializations/mathematics-machine-learning-data-science)). Ã‰tant en master Data & IA, ces cours me seront essentiels, notamment pour mon objectif Ã  long terme de crÃ©er mon propre modÃ¨le d'IA.

---

### ğŸ—ï¸ Architecture & Conception

Mes rÃ©flexions sur l'architecture matÃ©rielle et logicielle ont beaucoup Ã©voluÃ©.

- **SchÃ©ma d'architecture ROS** : J'avais rÃ©alisÃ© un premier schÃ©ma de mon architecture ROS. Ce n'Ã©tait qu'une base et absolument pas une solution finale.
  ![SchÃ©ma architecture](schemas/schema2.png)
- **Abandon du Lidar** : Entre temps j'ai pris la dÃ©cision de ne plus utiliser de capteur Lidar (comme le TF-Luna).
  - La raison principale est que la camÃ©ra stÃ©rÃ©o sera suffisante pour gÃ©nÃ©rer une **DepthMap** (carte de profondeur) via OpenCV.
  - De plus, le Lidar nÃ©cessitait un alignement trÃ¨s prÃ©cis (angle < 2Â°) entre la camÃ©ra, la pince et l'objet.
  - Mes premiers tests ont montrÃ© que le robot n'est pas assez prÃ©cis ou robuste pour cela. Le poids, la latence et la prÃ©cision des servos provoquaient des oscillations (va-et-vient) lors de la tentative de calibrage, sans jamais y parvenir. Je vais donc me concentrer uniquement sur la camÃ©ra stÃ©rÃ©o.
- **Positionnement de la camÃ©ra** : Je prÃ©vois d'installer la camÃ©ra stÃ©rÃ©o juste en dessous de la pince, fixÃ©e sur le servo qui contrÃ´le cette derniÃ¨re.
  ![SchÃ©ma positionnement camera](schemas/schema3.png)

---

### ğŸ’° MatÃ©riel & Budget

Le budget estimÃ© est respectÃ© avec les achats suivants :

| Ã‰quipement                | Prix      | Lien                                                                            |
| ------------------------- | --------- | ------------------------------------------------------------------------------- |
| Bras robotique xArm ESP32 | 229.99â‚¬   | [Hiwonder](https://www.hiwonder.com/products/xarm-esp32?variant=39662930067543) |
| CamÃ©ra USB stÃ©rÃ©o ELP     | 125â‚¬      | [Amazon](https://www.amazon.fr/dp/B07FT2GKZS)                                   |
| Raspberry Pi              | Ã€ acheter |                                                                                 |
| **Total (actuel)**        | **~355â‚¬** |                                                                                 |

Ces achats rentrent bien dans le budget estimÃ© de 400â‚¬ que j'avais mentionnÃ© prÃ©cÃ©demment.

---

### ğŸ—ºï¸ Grandes Lignes & Objectifs (Mise Ã  jour)

Avec ces nouveaux Ã©lÃ©ments, les grandes lignes du projet se prÃ©cisent :

1.  **Objectif** : Attraper un objet (une balle de tennis pour commencer) de maniÃ¨re autonome.
2.  **IA (Vision)** : J'utiliserai **YOLOv8** (pour ses performances) pour la dÃ©tection d'objet.
3.  **Vision (Profondeur)** : La **camÃ©ra stÃ©rÃ©o** et **OpenCV** me permettront de gÃ©nÃ©rer une DepthMap et d'obtenir les **coordonnÃ©es 3D** de l'objet.
4.  **Mouvement** : J'apprendrai Ã  utiliser les fichiers **URDF** (pour modÃ©liser le robot) et la bibliothÃ¨que **MoveIt** (pour planifier la trajectoire) afin de dÃ©terminer le chemin optimal pour saisir l'objet.
5.  **Futur** : Ã€ terme, je souhaite crÃ©er mon propre modÃ¨le d'IA, potentiellement basÃ© sur de l'**apprentissage par renforcement** (soit sur la reconnaissance de l'objet, soit sur la rÃ©ussite de la saisie complÃ¨te).

---

### ğŸ”œ Prochaines Ã©tapes

Maintenant que les idÃ©es et les technologies sont plus claires :

- Attendre la livraison de ma camÃ©ra stÃ©rÃ©o.
- Continuer Ã  me former sur ROS2, notamment sur les **fichiers URDF** et **MoveIt** (via la mÃªme chaÃ®ne YouTube).
- Apprendre la modÃ©lisation 3D ou l'impression 3D pour crÃ©er un support de camÃ©ra adaptÃ©.
  - Je vais me rapprocher de l'association d'innovation de mon Ã©cole qui propose des formations.
  - Alternativement, j'adapterai un modÃ¨le 3D existant, comme [celui-ci](https://makerworld.com/en/models/27135-raspberry-camera-mount?from=search), et je contacterai son crÃ©ateur pour voir s'il est compatible.
- Refaire un schÃ©ma de l'architecture globale du projet plus dÃ©taillÃ©.
- Essayer de contacter Edouard Renard (instructeur en robotique) pour lui demander un avis sur mon architecture et mes idÃ©es avant de dÃ©velopper.

---

### ğŸ—“ï¸ 08/11/2025

L'avancement continue, avec des progrÃ¨s sur la partie vision et la modÃ©lisation du robot.

---

### ğŸ› ï¸ DÃ©veloppement & Vision

- **CamÃ©ra StÃ©rÃ©o** : J'ai bien reÃ§u ma camÃ©ra stÃ©rÃ©o ELP.
- **Script de test** : J'ai pu Ã©crire un premier script `utils/stereo_camera.py` qui me permet d'initialiser la camÃ©ra et d'afficher les flux vidÃ©o des deux camÃ©rras.

---

### ğŸ—ï¸ Architecture (Mise Ã  jour)

J'ai simplifiÃ© mon architecture principale pour la rendre plus rÃ©alisable pour une premiÃ¨re version.

- **Nouvelle architecture** :
  ![SchÃ©ma architecture](schemas/schema4.png)
- **Limites actuelles** : Dans cette configuration, le robot est incapable de faire du temps rÃ©el. Il devra attendre que l'objet Ã  ramasser soit immobile et qu'il le reste pendant toute la durÃ©e du dÃ©placement.
- **Objectif V1** : Je vais m'en tenir Ã  cette approche pour la premiÃ¨re version du projet. La correction de trajectoire en temps rÃ©el sera une amÃ©lioration pour le futur.

---

### ğŸ¦¾ ModÃ©lisation & URDF

La crÃ©ation du jumeau numÃ©rique du robot dans ROS2 a Ã©tÃ© une Ã©tape majeure et complexe.

- **Fichier URDF** : J'ai appris Ã  Ã©crire un fichier `.xacro` et Ã  construire un package ROS2.
- **ModÃ¨le 3D** : L'entreprise Hiwonder m'a fourni le fichier `.stp` de mon robot, que j'ai ouvert via **Fusion360**.
- **Assemblage** : J'ai dÃ» trier et assembler les 309 piÃ¨ces de base en "components" logiques (pince, base, limb1, etc.) et crÃ©er les joints entre eux.
- **DifficultÃ© majeure** : Le script d'export par dÃ©faut ([fusion2urdf](https://github.com/syuntoku14/fusion2urdf/tree/master)) n'accepte pas les joints "as-built" de Fusion. Or, je ne pouvais pas utiliser de joints simples, car les piÃ¨ces Ã©taient dÃ©jÃ  assemblÃ©es dans le fichier `.stp` de base.
- **Solution** : AprÃ¨s 3 jours de recherches, j'ai trouvÃ© une [issue GitHub](https://github.com/syuntoku14/fusion2urdf/issues/78) dÃ©crivant le mÃªme problÃ¨me.
  - Un grand merci Ã  **Colin Fuelberth** ([@Infinite-Echo](https://github.com/Infinite-Echo)) qui a forkÃ© et adaptÃ© le script pour supporter les joints "as-built" !
  - **Script utilisÃ©** : [Infinite-Echo/ROS2_fusion2URDF](https://github.com/Infinite-Echo/ROS2_fusion2URDF/tree/URDF_Exporter_asBuilt_Support)
- **RÃ©sultat** : J'ai enfin pu exporter un package ROS2 complet avec un fichier `.xacro` qui dÃ©crit mon bras robot qui se trouve dans le dossier `modelisations/robot/xArm32_description`. Ce URDF ne prend pas en compte la fermeture et l'ouverture de la pince. Je l'adapterai au moment venu, l'objectif premier Ã©tait d'avoir les bases et d'apprendre le logiciel Fusion.

---

### ğŸ”œ Prochaines Ã©tapes

- Je suis actuellement un tutoriel **Blender** qui me permettra Ã  l'avenir de modÃ©liser et d'imprimer en 3D mon propre support de camÃ©ra.
- **Passer via Docker ou Linux** (avec un dual boot) pour ROS2. Je suis beaucoup trop limitÃ© avec mon Windows qui ne me facilite pas la tÃ¢che, surtout pour ouvrir et visualiser mon `.xacro`.

---

### ğŸ—“ï¸ 14/11/2025

J'ai continuÃ© ces derniers jours sur le modÃ¨le URDF. J'ai dÃ©crit les joints entre les segments (_limbs_) du bras avec des limites, Ã  l'exception de la pince car comprendre les _mimics_ est un peu trop complexe pour moi pour le moment. Je contrÃ´lerai sÃ»rement la pince via l'ESP32 directement.

Dans le dossier `app`, le premier package du `joint_state_publisher_node` est fonctionnel avec les instructions.
J'ai notamment effectuÃ© le changement vers Docker pour ROS2 Humble mais je rencontre un problÃ¨me d'images par seconde (FPS) pendant la simulation sous RViz ou Gazebo, ce qui rend l'expÃ©rience utilisateur dÃ©sagrÃ©able.

Vous trouverez ici un zip contenant le `.stp` fourni par Hiwonder et le fichier Fusion360 que j'ai Ã©ditÃ© si des modifications ou des amÃ©liorations sont Ã  faire : [Lien Google Drive](https://drive.google.com/file/d/1qIVWolMBeF4Z5x8Bm8aadgIRzJTUaZLs/view?usp=sharing)

GrÃ¢ce Ã  cette Ã©tape importante, je peux visualiser les articulations principales du bras via RViz ou Gazebo.
Je ne pense pas me servir de Gazebo dans ce projet.

![RViz rÃ©sultat](schemas/schema5.gif)

---

### ğŸ”œ Prochaines Ã©tapes

- Je suis actuellement un tutoriel **Blender** qui me permettra Ã  l'avenir de modÃ©liser et d'imprimer en 3D mon propre support de camÃ©ra.
- Me renseigner sur **ros2_control** et **MoveIt**.
- Apprendre Ã  utiliser des _mimic joints_ qui me permettraient de faire bouger la pince dans RViz Ã©galement. J'ai dÃ©jÃ  commencÃ© Ã  crÃ©er les joints nÃ©cessaires sur Fusion pour cela, mais le format URDF n'accepte pas les boucles de joints fermÃ©es.

![Fusion360 pince visualisation](schemas/schema6.gif)

---
