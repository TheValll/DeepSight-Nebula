# ğŸ¤–âœ¨ DeepSight-Nebula

**DeepSight-Nebula est un projet alliant Robotique & Intelligence Artificielle.**

---

## ğŸŒŒ Pourquoi le nom _DeepSight-Nebula_ ?

- ğŸ” _DeepSight_ : fait rÃ©fÃ©rence Ã  la **profondeur**, en Ã©cho au systÃ¨me de vision de ce projet.
- ğŸŒ  _Nebula_ : rÃ©fÃ©rence aux **nÃ©buleuses** objets cÃ©leste afin de reflÃ©ter ma passion pour lâ€™espace.

---

## ğŸ¯ Objectifs de DeepSight-Nebula

Le projet a plusieurs ambitions :

- ğŸš€ Mâ€™amÃ©liorer en **robotique** et en **IA** de maniÃ¨re gÃ©nÃ©rale.
- ğŸ“š Servir de base pour un **mÃ©moire de fin de master** en _Big Data & IA_, et potentiellement prÃ©parer un **PhD en Robotique & IA**.
- ğŸ¦¾ Concevoir un systÃ¨me capable de :
  - Piloter un **bras robotique** via une interface (comme un Raspberry Pi ğŸ“).
  - Utiliser un systÃ¨me de **vision** avec camÃ©ras et capteurs.
  - DÃ©ployer un **modÃ¨le dâ€™IA** pour analyser les objets prÃ©sents dans son champ de vision.
  - Ramasser les objets dÃ©tectÃ©s **de maniÃ¨re autonome**â€¦ le tout avec un budget raisonnable ğŸ’°.

---

## ğŸ“ Objectif de ce README

Ce document a deux fonctions principales :

- ğŸ“– RÃ©sumer et clarifier le but du projet pour moi et pour les lecteurs.
- ğŸ—’ï¸ Servir de **journal de bord** pour documenter mes recherches et dÃ©veloppements.

Jâ€™y consignerai :

- Des images ğŸ–¼ï¸
- Des schÃ©mas ğŸ“
- Des explications dÃ©taillÃ©es ğŸ“„
- Des solutions aux problÃ¨mes rencontrÃ©s ğŸ§©
- â€¦ et peut-Ãªtre mÃªme des questions que je me poserai en chemin ğŸ¤”

Je mâ€™appuierai sur **ChatGPT** pour mâ€™aider Ã  reformuler et amÃ©liorer ce document tout au long du projet.

---

## ğŸ“” Journal de bord

### ğŸ—“ï¸ 22/07/2025

Jâ€™ai dÃ©butÃ© mes recherches en rÃ©flÃ©chissant Ã  la conception et Ã  la faisabilitÃ© de ce projet.

ğŸ”¹ Dans un premier temps, je me suis penchÃ© sur le choix du bras robotique mais jâ€™ai vite Ã©tÃ© freinÃ© par les prix des modÃ¨les milieu de gamme.  
ğŸ”¹ Ensuite, je me suis plutÃ´t orientÃ© vers la **dÃ©tection et la vision par capteurs** en explorant plusieurs options :

- Lidar 2D ou 3D
- CamÃ©ra RGB-D
- CamÃ©ra stÃ©rÃ©o
- Ou une combinaison de tout cela

Comme je souhaite utiliser un **Raspberry Pi** (qui me servira aussi pour dâ€™autres projets), jâ€™en ai conclu aprÃ¨s plusieurs heures de recherche que je pourrais partir sur :

- ğŸ¥ Une **camÃ©ra ELP stÃ©rÃ©o** Ã  ~125â€¯â‚¬  
  [Lien Amazon](https://www.amazon.fr/ELP-distorsion-Synchronisation-dordinateur-Raspberry/dp/B07FT2GKZS?source=ps-sl-shoppingads-lpcontext&ref_=fplfs&psc=1&smid=A1XYWUUU38OZI5&gQT=1)
- ğŸ“¡ Un **capteur TF-Luna** Ã  ~29â€¯â‚¬  
  [Lien Amazon](https://www.amazon.fr/youyeetoo-TF-Luna-Distance-d%C3%A9tection-industrielle/dp/B088BBJ9SQ)

ğŸ‘‰ Je nâ€™ai pas encore pris la dÃ©cision de les acheter mais cela me donne une idÃ©e de la faisabilitÃ© et du budget.

ğŸ‘‰ Je n'ai Ã©galement pas choisi l'Intel RealSense (camÃ©ra stÃ©rÃ©o la plus connue) car ses dimensions sont trop grandes pour Ãªtre fixÃ©es sur un bras robot.

---

### ğŸ”§ RÃ©flexions techniques

ğŸ’¡ Jâ€™ai rapidement compris que la difficultÃ© majeure sera la **calibration et la fusion des donnÃ©es** issues du lidar et des camÃ©ras.  
Je pars donc sur lâ€™idÃ©e dâ€™utiliser **Python ou C++ avec OpenCV (cv2)** avec une prÃ©fÃ©rence pour Python notamment pour lâ€™intÃ©gration des modÃ¨les dâ€™IA.

- OpenCV me permettra de fusionner le flux des deux camÃ©ras et dâ€™obtenir un output image exploitable.
- Câ€™est ici que lâ€™IA entre en jeu : aprÃ¨s quelques recherche je vais probablement utiliser le modÃ¨le **YOLOWorld**, puis Ã  terme entraÃ®ner mon propre modÃ¨le sur un dataset personnalisÃ© (plus tard car câ€™est trÃ¨s chronophage).

Le modÃ¨le dÃ©tectera un objet particulier sur le flux vidÃ©o et retournera ses coordonnÃ©es **x** et **y**.  
Si aucun objet nâ€™est trouvÃ© dans le champ de vision le bras pourra bouger sur son axe pour balayer lâ€™espace.

---

### ğŸ“ Calibration & prÃ©cision

- Il faudra calibrer lâ€™ensemble (camÃ©ras + lidar) via un systÃ¨me de **calibrage extrinsÃ¨que** que je ne maÃ®trise pas encore.
- Pour commencer, je compte utiliser un **servo SG90 + Arduino** pour ajuster lâ€™axe de la camÃ©ra afin de rÃ©duire lâ€™angle entre lâ€™axe central de la camÃ©ra et lâ€™objet Ã  environ 2Â° (lâ€™angle max du TF-Luna placÃ© juste en dessous de la camÃ©ra). Cela permettra de vÃ©rifier la profondeur avec plus de prÃ©cision grÃ¢ce au lidar.
- Je recalculerai ensuite les coordonnÃ©es **x**, **y** de lâ€™objet en tenant compte de la calibration.
- Dans le futur le SG90 sera remplacÃ© par un mouvement de la base du bras.

---

### ğŸ¤– Mouvement & trajectoire

Avec les coordonnÃ©es **x, y, z** je pourrai dÃ©terminer un chemin pour le bras afin quâ€™il rÃ©cupÃ¨re lâ€™objet sans doute en utilisant des vecteurs et des contrÃ´leurs.  
Ces rÃ©flexions mâ€™ont permis de voir que jâ€™ai encore du temps et des alternatives avant dâ€™investir dans un Raspberry Pi et un bras robot.

---

### ğŸ’° Budget estimÃ©

| Ã‰quipement                     | Prix approx. |
| ------------------------------ | ------------ |
| CamÃ©ra stÃ©rÃ©o ELP              | 125â€¯â‚¬        |
| Lidar TF-Luna                  | 29â€¯â‚¬         |
| Bras robot (entrÃ©e/moyen)      | 60â€“200â€¯â‚¬     |
| Raspberry Pi (+ alim, modules) | 150â€¯â‚¬        |

**Total estimÃ© : ~400â€¯â‚¬**

---

### ğŸ”œ Prochaines Ã©tapes

Je vais commencer par :

- Me familiariser avec le modÃ¨le **YOLOWorld** et OpenCV.
- Utiliser ma camÃ©ra actuelle (Logitech StreamCam) pour expÃ©rimenter.
- Je dois trouver un moyen de faire tenir ma camÃ©ra sur le SG90 de maniÃ¨re stable.
- Ajuster lâ€™axe de la camÃ©ra pour entrer dans la plage de vision du lidar (~2Â°) aprÃ¨s avoir dÃ©tectÃ© un objet avec **YOLOWorld** et calculer les premiÃ¨res coordonnÃ©es dans OpenCV.

---

### ğŸ—ºï¸ SchÃ©ma

La premiÃ¨re grosse Ã©tape de ce projet sera dâ€™apprendre Ã  utiliser, avec ce que jâ€™ai dÃ©jÃ  Ã  disposition, les outils (OpenCV, modÃ¨le IA, etc.), puis de passer Ã  lâ€™achat du lidar et de la camÃ©ra.  
Lâ€™objectif de cette Ã©tape est dâ€™arriver Ã  obtenir les **coordonnÃ©es 3D dâ€™un objet** dans le champ de vision de la camÃ©ra.

Voici le schÃ©ma de la reprÃ©sentation de cette premiÃ¨re Ã©tape que jâ€™ai en tÃªteâ€¯:

![SchÃ©ma Ã©tape 1](schemas/schema1.png)

---

### ğŸ—“ï¸ 03/11/2025

Cela fait 4 mois que je n'ai pas rÃ©digÃ© de rÃ©capitulatif sur ce document, principalement par manque de temps et en raison d'une phase d'apprentissage intense.

---

### ğŸ› ï¸ Apprentissage & DÃ©veloppement

Ces derniers mois ont Ã©tÃ© consacrÃ©s Ã  l'acquisition de nouvelles compÃ©tences et au premier dÃ©veloppement sur le robot.

- J'ai fait l'achat du bras robot xArm Esp32 de chez [Hiwonder](https://www.hiwonder.com/products/xarm-esp32?variant=39662930067543)

- **Formation ROS2** : J'ai commencÃ© Ã  me former Ã  **ROS2** en suivant ce [tutoriel YouTube](https://www.youtube.com/watch?v=Gg25GfA456o&t). J'ai pu apprÃ©hender les concepts de _nodes_, _publisher_, _subscriber_, _client_, _server_ et _actions_.
- **ContrÃ´le du Robot** : Le bras **xArm ESP32** n'ayant pas de logiciel constructeur facilitant le dÃ©veloppement, j'ai dÃ» apprendre Ã  rÃ©cupÃ©rer les informations transitant via les ports USB.
  - Ã€ l'aide du logiciel **COM8 Monitoring Session**, j'ai pu analyser les commandes envoyÃ©es par le logiciel basique du robot.
  - J'ai ainsi pu comprendre quelles commandes envoyer pour le piloter. Vous trouverez ce dÃ©veloppement dans le fichier `utils/xarm_esp32_init.py` qui gÃ¨re les actions de base.
- **MathÃ©matiques & IA** : Je suis actuellement des cours de mathÃ©matiques sur Coursera ([Mathematics for Machine Learning and Data Science de DeepLearning.AI](https://www.coursera.org/specializations/mathematics-machine-learning-data-science)). Ã‰tant en master Data & IA, ces cours me seront essentiels, notamment pour mon objectif Ã  long terme de crÃ©er mon propre modÃ¨le d'IA.

---

### ğŸ—ï¸ Architecture & Conception

Mes rÃ©flexions sur l'architecture matÃ©rielle et logicielle ont beaucoup Ã©voluÃ©.

- **SchÃ©ma d'architecture ROS** : J'avais rÃ©alisÃ© un premier schÃ©ma de mon architecture ROS. Ce n'Ã©tait qu'une base et absolument pas une solution finale.
  ![SchÃ©ma architecture](schemas/schema2.png)
- **Abandon du Lidar** : Entre temps j'ai pris la dÃ©cision de ne plus utiliser de capteur Lidar (comme le TF-Luna).
  - La raison principale est que la camÃ©ra stÃ©rÃ©o sera suffisante pour gÃ©nÃ©rer une **DepthMap** (carte de profondeur) via OpenCV.
  - De plus, le Lidar nÃ©cessitait un alignement trÃ¨s prÃ©cis (angle < 2Â°) entre la camÃ©ra, la pince et l'objet.
  - Mes premiers tests ont montrÃ© que le robot n'est pas assez prÃ©cis ou robuste pour cela. Le poids, la latence et la prÃ©cision des servos provoquaient des oscillations (va-et-vient) lors de la tentative de calibrage, sans jamais y parvenir. Je vais donc me concentrer uniquement sur la camÃ©ra stÃ©rÃ©o.
- **Positionnement de la camÃ©ra** : Je prÃ©vois d'installer la camÃ©ra stÃ©rÃ©o juste en dessous de la pince, fixÃ©e sur le servo qui contrÃ´le cette derniÃ¨re.
  ![SchÃ©ma positionnement camera](schemas/schema3.png)

---

### ğŸ’° MatÃ©riel & Budget

Le budget estimÃ© est respectÃ© avec les achats suivants :

| Ã‰quipement                | Prix      | Lien                                                                            |
| ------------------------- | --------- | ------------------------------------------------------------------------------- |
| Bras robotique xArm ESP32 | 229.99â‚¬   | [Hiwonder](https://www.hiwonder.com/products/xarm-esp32?variant=39662930067543) |
| CamÃ©ra USB stÃ©rÃ©o ELP     | 125â‚¬      | [Amazon](https://www.amazon.fr/dp/B07FT2GKZS)                                   |
| Raspberry Pi              | Ã€ acheter |                                                                                 |
| **Total (actuel)**        | **~355â‚¬** |                                                                                 |

Ces achats rentrent bien dans le budget estimÃ© de 400â‚¬ que j'avais mentionnÃ© prÃ©cÃ©demment.

---

### ğŸ—ºï¸ Grandes Lignes & Objectifs (Mise Ã  jour)

Avec ces nouveaux Ã©lÃ©ments, les grandes lignes du projet se prÃ©cisent :

1.  **Objectif** : Attraper un objet (une balle de tennis pour commencer) de maniÃ¨re autonome.
2.  **IA (Vision)** : J'utiliserai **YOLOv8** (pour ses performances) pour la dÃ©tection d'objet.
3.  **Vision (Profondeur)** : La **camÃ©ra stÃ©rÃ©o** et **OpenCV** me permettront de gÃ©nÃ©rer une DepthMap et d'obtenir les **coordonnÃ©es 3D** de l'objet.
4.  **Mouvement** : J'apprendrai Ã  utiliser les fichiers **URDF** (pour modÃ©liser le robot) et la bibliothÃ¨que **MoveIt** (pour planifier la trajectoire) afin de dÃ©terminer le chemin optimal pour saisir l'objet.
5.  **Futur** : Ã€ terme, je souhaite crÃ©er mon propre modÃ¨le d'IA, potentiellement basÃ© sur de l'**apprentissage par renforcement** (soit sur la reconnaissance de l'objet, soit sur la rÃ©ussite de la saisie complÃ¨te).

---

### ğŸ”œ Prochaines Ã©tapes

Maintenant que les idÃ©es et les technologies sont plus claires :

- Attendre la livraison de ma camÃ©ra stÃ©rÃ©o.
- Continuer Ã  me former sur ROS2, notamment sur les **fichiers URDF** et **MoveIt** (via la mÃªme chaÃ®ne YouTube).
- Apprendre la modÃ©lisation 3D ou l'impression 3D pour crÃ©er un support de camÃ©ra adaptÃ©.
  - Je vais me rapprocher de l'association d'innovation de mon Ã©cole qui propose des formations.
  - Alternativement, j'adapterai un modÃ¨le 3D existant, comme [celui-ci](https://makerworld.com/en/models/27135-raspberry-camera-mount?from=search), et je contacterai son crÃ©ateur pour voir s'il est compatible.
- Refaire un schÃ©ma de l'architecture globale du projet plus dÃ©taillÃ©.
- Essayer de contacter Edouard Renard (instructeur en robotique) pour lui demander un avis sur mon architecture et mes idÃ©es avant de dÃ©velopper.

---

### ğŸ—“ï¸ 08/11/2025

L'avancement continue, avec des progrÃ¨s sur la partie vision et la modÃ©lisation du robot.

---

### ğŸ› ï¸ DÃ©veloppement & Vision

- **CamÃ©ra StÃ©rÃ©o** : J'ai bien reÃ§u ma camÃ©ra stÃ©rÃ©o ELP.
- **Script de test** : J'ai pu Ã©crire un premier script `utils/stereo_camera.py` qui me permet d'initialiser la camÃ©ra et d'afficher les flux vidÃ©o des deux camÃ©rras.

---

### ğŸ—ï¸ Architecture (Mise Ã  jour)

J'ai simplifiÃ© mon architecture principale pour la rendre plus rÃ©alisable pour une premiÃ¨re version.

- **Nouvelle architecture** :
  ![SchÃ©ma architecture](schemas/schema4.png)
- **Limites actuelles** : Dans cette configuration, le robot est incapable de faire du temps rÃ©el. Il devra attendre que l'objet Ã  ramasser soit immobile et qu'il le reste pendant toute la durÃ©e du dÃ©placement.
- **Objectif V1** : Je vais m'en tenir Ã  cette approche pour la premiÃ¨re version du projet. La correction de trajectoire en temps rÃ©el sera une amÃ©lioration pour le futur.

---

### ğŸ¦¾ ModÃ©lisation & URDF

La crÃ©ation du jumeau numÃ©rique du robot dans ROS2 a Ã©tÃ© une Ã©tape majeure et complexe.

- **Fichier URDF** : J'ai appris Ã  Ã©crire un fichier `.xacro` et Ã  construire un package ROS2.
- **ModÃ¨le 3D** : L'entreprise Hiwonder m'a fourni le fichier `.stp` de mon robot, que j'ai ouvert via **Fusion360**.
- **Assemblage** : J'ai dÃ» trier et assembler les 309 piÃ¨ces de base en "components" logiques (pince, base, limb1, etc.) et crÃ©er les joints entre eux.
- **DifficultÃ© majeure** : Le script d'export par dÃ©faut ([fusion2urdf](https://github.com/syuntoku14/fusion2urdf/tree/master)) n'accepte pas les joints "as-built" de Fusion. Or, je ne pouvais pas utiliser de joints simples, car les piÃ¨ces Ã©taient dÃ©jÃ  assemblÃ©es dans le fichier `.stp` de base.
- **Solution** : AprÃ¨s 3 jours de recherches, j'ai trouvÃ© une [issue GitHub](https://github.com/syuntoku14/fusion2urdf/issues/78) dÃ©crivant le mÃªme problÃ¨me.
  - Un grand merci Ã  **Colin Fuelberth** ([@Infinite-Echo](https://github.com/Infinite-Echo)) qui a forkÃ© et adaptÃ© le script pour supporter les joints "as-built" !
  - **Script utilisÃ©** : [Infinite-Echo/ROS2_fusion2URDF](https://github.com/Infinite-Echo/ROS2_fusion2URDF/tree/URDF_Exporter_asBuilt_Support)
- **RÃ©sultat** : J'ai enfin pu exporter un package ROS2 complet avec un fichier `.xacro` qui dÃ©crit mon bras robot qui se trouve dans le dossier `modelisations/robot/xArm32_description`. Ce URDF ne prend pas en compte la fermeture et l'ouverture de la pince. Je l'adapterai au moment venu, l'objectif premier Ã©tait d'avoir les bases et d'apprendre le logiciel Fusion.

---

### ğŸ”œ Prochaines Ã©tapes

- Je suis actuellement un tutoriel **Blender** qui me permettra Ã  l'avenir de modÃ©liser et d'imprimer en 3D mon propre support de camÃ©ra.
- **Passer via Docker ou Linux** (avec un dual boot) pour ROS2. Je suis beaucoup trop limitÃ© avec mon Windows qui ne me facilite pas la tÃ¢che, surtout pour ouvrir et visualiser mon `.xacro`.

---

### ğŸ—“ï¸ 14/11/2025

J'ai continuÃ© ces derniers jours sur le modÃ¨le URDF. J'ai dÃ©crit les joints entre les segments (_limbs_) du bras avec des limites, Ã  l'exception de la pince car comprendre les _mimics_ est un peu trop complexe pour moi pour le moment. Je contrÃ´lerai sÃ»rement la pince via l'ESP32 directement.

Dans le dossier `app`, le premier package du `joint_state_publisher_node` est fonctionnel avec les instructions.
J'ai notamment effectuÃ© le changement vers Docker pour ROS2 Humble mais je rencontre un problÃ¨me d'images par seconde (FPS) pendant la simulation sous RViz ou Gazebo, ce qui rend l'expÃ©rience utilisateur dÃ©sagrÃ©able.

Vous trouverez ici un zip contenant le `.stp` fourni par Hiwonder et le fichier Fusion360 que j'ai Ã©ditÃ© si des modifications ou des amÃ©liorations sont Ã  faire : [Lien Google Drive](https://drive.google.com/file/d/1qIVWolMBeF4Z5x8Bm8aadgIRzJTUaZLs/view?usp=sharing)

GrÃ¢ce Ã  cette Ã©tape importante, je peux visualiser les articulations principales du bras via RViz ou Gazebo.
Je ne pense pas me servir de Gazebo dans ce projet.

![RViz rÃ©sultat](schemas/schema5.gif)

---

### ğŸ”œ Prochaines Ã©tapes

- Je suis actuellement un tutoriel **Blender** qui me permettra Ã  l'avenir de modÃ©liser et d'imprimer en 3D mon propre support de camÃ©ra.
- Me renseigner sur **ros2_control** et **MoveIt**.
- Apprendre Ã  utiliser des _mimic joints_ qui me permettraient de faire bouger la pince dans RViz Ã©galement. J'ai dÃ©jÃ  commencÃ© Ã  crÃ©er les joints nÃ©cessaires sur Fusion pour cela, mais le format URDF n'accepte pas les boucles de joints fermÃ©es.

![Fusion360 pince visualisation](schemas/schema6.gif)

---
